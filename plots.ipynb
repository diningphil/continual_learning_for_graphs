{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT FOR CL-DGN WITH NEW API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill this\n",
    "RUN_NAME = \"final_run\"\n",
    "RUN_TEMPLATE = \"final_run{}\"\n",
    "TRAINING_RESULTS_NAME = \"training_results.csv\"\n",
    "INTERMEDIATE_RESULTS_NAME = \"intermediate_results.csv\"\n",
    "RESULT_BASE_PATH = \"/data/cossu/CL-DGN\" # BASE PATH TO GOOGLE DRIVE FOLDER\n",
    "SAVE_IMG_PATH = \"/data/cossu/CL-DGN/PLOTS/\" # PATH TO SAVE PLOTS\n",
    "REG_SUFFIX = '_REG'\n",
    "\n",
    "# used to get labels for plots instead of ugly long model names\n",
    "models_to_label = {\n",
    "    'SuperpixelsBaseline': 'MLP',\n",
    "    'GraphSAGESuperpixels': 'DGN',\n",
    "    f'GraphSAGESuperpixels{REG_SUFFIX}': f'DGN{REG_SUFFIX}',\n",
    "    'GraphSAGEOGBGPPA': 'DGN-OGBG',\n",
    "    f'GraphSAGEOGBGPPA{REG_SUFFIX}': f'DGN-OGBG{REG_SUFFIX}',\n",
    "    'OGBGBaseline': 'MLP-OGBG'\n",
    "}\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "markers = [\"o\",\"v\",\"^\",\"<\",\">\",\"8\",\"s\",\"p\",\"P\",\"*\", \".\", \"h\",\"H\",\"+\",\"x\",\"X\",\"D\",\"d\"]\n",
    "linestyles = ['--', '-', '-.', ':']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_csv(fpath, write_new_file=False):\n",
    "\n",
    "    with open(fpath,'r') as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    dest_text = []\n",
    "    parenthesis = False\n",
    "    first_number = True\n",
    "    digits = ['0','1','2','3','4','5','6','7','8','9', '.']\n",
    "    \n",
    "    for el in text:\n",
    "        if el == '{':\n",
    "            parenthesis=True\n",
    "        elif el == '}':\n",
    "            parenthesis=False\n",
    "            first_number = True\n",
    "        elif el == ',' and parenthesis:\n",
    "            first_number = False\n",
    "        else:\n",
    "            if not parenthesis:\n",
    "                dest_text.append(el)\n",
    "            else:\n",
    "                if el in digits and first_number:\n",
    "                    dest_text.append(el)\n",
    "                    \n",
    "    dest_text = ''.join(dest_text)\n",
    "    \n",
    "    if write_new_file:\n",
    "        with open(fpath, 'w') as f:\n",
    "            f.write(dest_text)\n",
    "            \n",
    "    return dest_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_training_mean_std(root):\n",
    "    \n",
    "    selected_columns = ['train_loss', 'main_score', 'val_loss', 'main_score.1']\n",
    "\n",
    "    num_runs = len([el for el in os.listdir(root) if el.startswith(RUN_NAME)])\n",
    "    data_gathered = None\n",
    "    for i in range(1,num_runs+1):\n",
    "        cur_file = os.path.join(root, RUN_TEMPLATE.format(i), TRAINING_RESULTS_NAME)\n",
    "        \n",
    "        #preprocess_csv(cur_file, write_new_file=True)\n",
    "        \n",
    "        data = pd.read_csv(cur_file)\n",
    "        try:\n",
    "            data = np.expand_dims(data[selected_columns].values, axis=0)\n",
    "        except:\n",
    "            print(i)\n",
    "            print(data.head())\n",
    "            raise ValueError()\n",
    "\n",
    "        if data_gathered is None:\n",
    "            data_gathered = data\n",
    "        else:\n",
    "            data_gathered = np.concatenate((data_gathered, data), axis=0)\n",
    "    \n",
    "    averages = np.average(data_gathered, axis=0)\n",
    "    stds = np.std(data_gathered, axis=0)\n",
    "    \n",
    "    train_loss_mean, val_loss_mean, train_acc_mean, val_acc_mean = averages[:,0], averages[:,1], averages[:,2], averages[:,3]\n",
    "    train_loss_std, val_loss_std, train_acc_std, val_acc_std = stds[:,0], stds[:,1], stds[:,2], stds[:,3]\n",
    "    return (train_loss_mean, train_acc_mean, val_loss_mean, val_acc_mean), \\\n",
    "           (train_loss_std, train_acc_std, val_loss_std, val_acc_std)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intermediate_mean_std(root):\n",
    "\n",
    "    selected_columns = ['loss', 'main_score']\n",
    "    \n",
    "    num_runs = len([el for el in os.listdir(root) if el.startswith(RUN_NAME)])\n",
    "    data_gathered = None\n",
    "    for i in range(1,num_runs+1):\n",
    "        cur_file = os.path.join(root, RUN_TEMPLATE.format(i), INTERMEDIATE_RESULTS_NAME)\n",
    "\n",
    "        #preprocess_csv(cur_file, write_new_file=True)\n",
    "\n",
    "        data = pd.read_csv(cur_file)\n",
    "        data = data[data['task_id'] == data['task_id'].max()] # choose last task\n",
    "        data = np.expand_dims(data[selected_columns].values, axis=0)\n",
    "        if data_gathered is None:\n",
    "            data_gathered = data\n",
    "        else:\n",
    "            data_gathered = np.concatenate((data_gathered, data), axis=0)\n",
    "        \n",
    "    averages = np.average(data_gathered, axis=0)\n",
    "    stds = np.std(data_gathered, axis=0)\n",
    "    \n",
    "    loss_mean, acc_mean = averages[:,0], averages[:,1]\n",
    "    loss_std, acc_std = stds[:,0], stds[:,1]\n",
    "    return (loss_mean, acc_mean), (loss_std, acc_std)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cmap(n, name='hsv'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUMERICAL RESULTS (NO PLOTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'GraphSAGEOGBGPPA'\n",
    "experiment = 'EWC'\n",
    "dataset = 'ogbg_ppa'\n",
    "ROOT = os.path.join(RESULT_BASE_PATH, \n",
    "                    f\"RESULTS_EWC/{model}_{dataset}_assessment/MODEL_ASSESSMENT/OUTER_FOLD_1\")\n",
    "\n",
    "avg, std = compute_training_mean_std(ROOT)\n",
    "print(avg)\n",
    "print()\n",
    "print(std)\n",
    "print()\n",
    "\n",
    "avg, std = compute_intermediate_mean_std(ROOT)\n",
    "print(avg)\n",
    "print()\n",
    "print(std)\n",
    "print()\n",
    "\n",
    "# mean and std over tasks (and over runs)\n",
    "print(np.mean(avg[1]))\n",
    "print()\n",
    "print(np.mean(std[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT AT INCREASING LEVEL OF MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill this\n",
    "mem_sizes = [50, 100, 200, 500, 1000, 2000]\n",
    "#models = ['SuperpixelsBaseline', 'GraphSAGESuperpixels', 'GraphSAGEOGBGPPA']\n",
    "models = ['GraphSAGEOGBGPPA']\n",
    "reg_models = ['GraphSAGEOGBGPPA']\n",
    "DATASET = 'ogbg_ppa'\n",
    "\n",
    "baseline_folder = os.path.join(RESULT_BASE_PATH, \"RESULTS_REHEARSAL_{}{}/{}\"+f\"_{DATASET}_assessment/MODEL_ASSESSMENT/OUTER_FOLD_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs = {}\n",
    "stds = {}\n",
    "for model in models:\n",
    "    avgs[model] = []\n",
    "    stds[model] = []\n",
    "    for mem_size in mem_sizes:\n",
    "        (_, avg_run), (_, std_run) = compute_intermediate_mean_std(baseline_folder.format(mem_size, '', model))\n",
    "        avg_task, std_task = np.mean(avg_run), np.mean(std_run)\n",
    "        avgs[model].append(avg_task)\n",
    "        stds[model].append(std_task)\n",
    "\n",
    "for model in reg_models:\n",
    "    avgs[model+REG_SUFFIX] = []\n",
    "    stds[model+REG_SUFFIX] = []\n",
    "    for mem_size in mem_sizes:\n",
    "        (_, avg_run), (_, std_run) = compute_intermediate_mean_std(baseline_folder.format(mem_size, REG_SUFFIX, model))\n",
    "        avg_task, std_task = np.mean(avg_run), np.mean(std_run)\n",
    "        avgs[model+REG_SUFFIX].append(avg_task)\n",
    "        stds[model+REG_SUFFIX].append(std_task)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = get_cmap(100, 'Set1')\n",
    "xcoords = list(range(2,len(mem_sizes)*2+1,2))\n",
    "plt.figure()\n",
    "for i, model in enumerate(avgs.keys()):\n",
    "    plt.errorbar(xcoords, avgs[model], yerr=stds[model], fmt=markers[i]+linestyles[i], label=models_to_label[model], c=cmap(i*20))\n",
    "plt.xticks(xcoords, [str(el) for el in mem_sizes])\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Memory size')\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(True)\n",
    "plt.title(DATASET)\n",
    "plt.savefig(os.path.join(SAVE_IMG_PATH, f'{DATASET}_rehearsal_memory.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAIRED PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill this\n",
    "#models = ['SuperpixelsBaseline', 'GraphSAGESuperpixels', 'GraphSAGEOGBGPPA'] \n",
    "models = ['GraphSAGEOGBGPPA']\n",
    "exp_category = 'RESULTS_REHEARSAL_1000'\n",
    "DATASET = 'ogbg_ppa'\n",
    "reg_models = ['GraphSAGEOGBGPPA']\n",
    "\n",
    "baseline_folder = os.path.join(RESULT_BASE_PATH, exp_category+\"{}/{}\"+f\"_{DATASET}_assessment/MODEL_ASSESSMENT/OUTER_FOLD_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_avgs = {}\n",
    "test_avgs = {}\n",
    "for model in models:\n",
    "    (_, _, _, val_acc), _ = compute_training_mean_std(baseline_folder.format('', model))    \n",
    "    (_, test_acc), _ = compute_intermediate_mean_std(baseline_folder.format('', model))\n",
    "    val_avgs[model] = val_acc\n",
    "    test_avgs[model] = test_acc\n",
    "\n",
    "for model in reg_models:\n",
    "    (_, _, _, val_acc), _ = compute_training_mean_std(baseline_folder.format(REG_SUFFIX, model))    \n",
    "    (_, test_acc), _ = compute_intermediate_mean_std(baseline_folder.format(REG_SUFFIX, model))\n",
    "    val_avgs[model+REG_SUFFIX] = val_acc\n",
    "    test_avgs[model+REG_SUFFIX] = test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = get_cmap(val_avgs[models[0]].shape[0], 'Set1')\n",
    "DIST = 20\n",
    "INTER_DIST = 5\n",
    "xcoords = list(range(8, (len(models)+len(reg_models))*DIST+1, DIST))\n",
    "plt.figure()\n",
    "plt.hlines(10, -3, xcoords[-1]+DIST, colors='gray', linewidth=2, linestyle=':', zorder=1, label='random')\n",
    "\n",
    "# plot pairs of points\n",
    "for i, model in enumerate(val_avgs.keys()):\n",
    "    for task_id in range(val_avgs[model].shape[0]):\n",
    "        plt.plot([xcoords[i]-INTER_DIST, xcoords[i]+INTER_DIST], [val_avgs[model][task_id], test_avgs[model][task_id]], c='k', linewidth=0.5, zorder=1)\n",
    "        plt.scatter([xcoords[i]-INTER_DIST, xcoords[i]+INTER_DIST], [val_avgs[model][task_id], test_avgs[model][task_id]], marker=markers[task_id], s=40, c=[cmap(task_id)], label='T'+str(task_id+1), zorder=2)\n",
    "\n",
    "# plot mean vector\n",
    "for i, model in enumerate(val_avgs.keys()):\n",
    "    left_mean = np.mean(val_avgs[model])\n",
    "    right_mean = np.mean(test_avgs[model])\n",
    "    plt.plot([xcoords[i]-INTER_DIST, xcoords[i]+INTER_DIST], [left_mean, right_mean], c='red', ls='--', linewidth=0.5, zorder=1)\n",
    "    plt.scatter([xcoords[i]-INTER_DIST, xcoords[i]+INTER_DIST], [left_mean, right_mean], marker='*', s=40, c='darkred', label='mean', zorder=2)\n",
    "\n",
    "\n",
    "# remove duplicated legend entries\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='best')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(-3,105)\n",
    "plt.xlim(0,xcoords[-1]+DIST)\n",
    "plt.xticks(xcoords, [models_to_label[el] for el in val_avgs.keys()])\n",
    "plt.grid(True)\n",
    "plt.title(DATASET)\n",
    "plt.savefig(os.path.join(SAVE_IMG_PATH, f'{exp_category}_{DATASET}_paired_plot.png'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
